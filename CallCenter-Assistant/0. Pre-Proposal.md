# Proposal: AI-powered Knowledge Centralization & Call Center Automation

## 1. Problem Statement
Legacy Call Center has stored **audio recordings in Japanese** between users and human agents.  
Current challenges:
- **Language barrier**: Not supported in other languages.
- **Knowledge lock-in**: Insights in recordings are not reusable.
- **Scalability issue**: Lack of human agents to support increasing user demand.

**Goal:** Transform legacy audio into a **multilingual, reusable knowledge base** and build an **AI-first call center system** with human-in-the-loop (HITL) for reliability and scalability.

---

## 2. Solution Overview

### Phase 1: Knowledge Centralization
- Convert Japanese audio → text via **ASR (Automatic Speech Recognition)**.
- Translate into other languages (VN/EN) via **NMT (Neural Machine Translation)**.
- Extract **knowledge objects**: QA pairs, summaries, SOPs.
- Store results in a **Knowledge Database** (construct under a knowledge graph) for retrieval.

### Phase 2: System Development
Design system to support different levels of human-AI collaboration:

1. **Proposal 1 – Human Enhancement**  
   AI assists agents with fast reference answers, but **agent remains primary responder**.  
   → Boosts **productivity**.

2. **Proposal 2 – Human Supervision**  
   AI generates answers, human agents **review/approve** before delivery.  
   → Balances **automation + control**.

3. **Proposal 3 – Human as Support**  
   AI answers directly; human pool is called **only for low-confidence or edge cases**.  
   → Maximizes **scalability**.

4. **Proposal 3 - Combine all 3 into an AI-first HITL framework** 
- Default: AI responds (Proposal 3).  
- Escalation: Human approves (Proposal 2).  
- Agents use AI as co-pilot in complex cases (Proposal 1).

---

## 3. Technical Architecture

### 3.1 Data Ingestion & Preprocessing
- Collect audio + metadata (call ID, agent ID, timestamp).
- Apply **noise reduction** and **speaker diarization**.
- Transcribe with **Japanese ASR** (e.g., Whisper fine-tuned on JP).

### 3.2 Translation & Normalization
- Detect language → standardize to Japanese → NMT to EN/VN.
- For core KB: apply **human post-edit** on translations.

### 3.3 Knowledge Extraction
- Parse transcripts into:
  - **QA pairs**
  - **Summaries**
  - **Intents / Entities**
- Add **metadata** (confidence score, source ID, timestamp).

### 3.4 Knowledge Indexing
- Chunk data (semantic/time-based).
- Generate **embeddings** → store in **Vector Database** (e.g., Pinecone, Qdrant, Milvus).
- Maintain **relational DB** for metadata & audit logs.

### 3.5 Runtime: RAG Pipeline
- **User query** → Semantic retrieval (top-k passages).
- **LLM** generates answer with retrieved context (reduces hallucination).
- **Citations** provided (links to transcript/audio segment).
- **Confidence scoring** determines escalation to human agent.

### 3.6 Human-in-the-loop
- **High confidence** → Bot answers directly.
- **Medium confidence / Policy-sensitive** → Route to human agent with AI-suggested answer.
- **Low confidence / Escalation request** → Assign to human agent pool.

### 3.7 Feedback & Continuous Learning
- Log interactions, approvals, rejections.
- Retrain/fine-tune ASR, NMT, ranker, and LLM prompts.
- Improve over time.

---

## 4. Implementation Roadmap

### Phase 0 – Preparation (2–4 weeks)
- Audit dataset (size, quality, compliance).
- Run **POC** on 500–1000 calls:
  - Evaluate ASR WER (Word Error Rate).
  - Test translation accuracy.

### Phase 1 – Centralization (4–8 weeks)
- Build pipeline: Audio → Transcript → QA pairs.
- Set up **Knowledge Database** (text + metadata).
- Deploy simple retrieval API + internal test UI.

### Phase 2 – RAG Assistant + HITL (6–10 weeks)
- Integrate **LLM + Retrieval** for answer generation.
- Add **confidence scoring & human fallback logic**.
- Build **agent UI** (AI-suggested answers with citations).

### Phase 3 – Multilingual & Scaling (8–12 weeks)
- Expand translation to EN/VN.
- Monitor ASR/NMT quality, fine-tune on domain-specific corpus.
- Define SLA & monitoring dashboards.

### Phase 4 – Continuous Ops (Ongoing)
- Collect feedback to refine models.
- Optimize latency, cost, and accuracy.
- Add A/B testing and reporting.

---

## 5. Metrics for Success
- **ASR quality**: WER target benchmark.  
- **Retrieval accuracy**: % queries where correct passage is in top-5.  
- **Automation rate**: % user queries resolved without human intervention.  
- **Escalation rate**: % queries needing human intervention.  
- **Agent productivity uplift**: Time saved per interaction.  
- **User CSAT**: Satisfaction scores.

---

## 6. Risks & Mitigation
| Risk | Mitigation |
|------|------------|
| Poor ASR due to noise/dialects | Fine-tune ASR, pre-process audio, diarization |
| Translation errors | Human post-edit for canonical KB |
| AI hallucinations | Use **RAG + citations + confidence thresholds** |
| Data privacy | Apply **PII redaction, encryption, access control** |
| Cost scaling | Hybrid infra: managed + self-host; optimize inference |

---

## 7. Team & Resource Estimate
- **Product Owner / PM**: 1  
- **ML Engineers (ASR, embeddings, retriever)**: 1–2  
- **Backend / Infra Engineer**: 1  
- **Frontend Engineer (Agent UI)**: 1  
- **Data Annotators (QA extraction, translation post-edit)**: 1–2  
- **DevOps/MLOps**: 1  

**Timeline to MVP:** ~3–4 months.

---

## 8. Expected Impact
- Unlocks **hidden knowledge** from legacy recordings.  
- Enables **multilingual support** (JP, EN, VN).  
- Reduces reliance on limited human agents.  
- Improves **response time, scalability, and consistency**.  
- Provides **continuous learning system** that gets better over time.

---

## 9. Next Steps
1. Select 500–1000 sample calls for **POC testing**.  
2. Evaluate ASR/NMT performance & select best models.  
3. Stand up **vector DB + retrieval pipeline**.  
4. Build minimal **RAG assistant with human-in-loop**.  
5. Measure automation rate & agent productivity uplift.
